# MapReAct-VLN: Map-Guided ReAct for Vision-Language Navigation

<div align="center">

**A ReAct-based VLM navigation system with dynamic semantic map guidance**

[![Python](https://img.shields.io/badge/Python-3.8-blue.svg)](https://www.python.org/)
[![Habitat](https://img.shields.io/badge/Habitat-v0.1.7-orange.svg)](https://github.com/facebookresearch/habitat-lab)

[Paper](https://arxiv.org/abs/2412.10137) | [Project](https://chenkehan21.github.io/CA-Nav-project/) | [Docs](docs/)

</div>

---

## ğŸ¯ Overview

MapReAct-VLN implements a **hierarchical ReAct framework** for Vision-Language Navigation in Continuous Environments (VLN-CE):

- **THOUGHT**: LLM decomposes tasks into subtasks with map context
- **ACT**: VLM executes actions guided by RGB + Detection + Local Map
- **REFLECT**: 360Â° scanning verifies progress and replans

**Key Features**:
- ğŸ—ºï¸ Dynamic semantic mapping with landmark tracking
- ğŸ¯ Constraint-aware subtask decomposition
- ğŸ”„ Adaptive replanning with 360Â° context
- ğŸ¤– Open-vocabulary object detection (GroundedSAM)

---

## ğŸ—ï¸ System Architecture

```
Instruction â†’ LLM (THOUGHT)
                â†“
            Subtask + Landmark
                â†“
          Update Map Markers
                â†“
            VLM (ACT)
                â†“
    RGB + Detection + Local Map â†’ Action
                â†“
          Execute & Update
                â†“
          Check Completion (REFLECT)
                â†“
        360Â° Scan â†’ Verify â†’ Replan
                â†“
            Loop until Goal
```

**Data Flow**:
```python
# THOUGHT Phase
llm_input = {
    "instruction": "Walk to the kitchen...",
    "images": [front_0Â°, left_90Â°, back_180Â°, right_270Â°],
    "maps": [global_map, local_map],
    "detected_objects": ["floor", "wall", "door", "table"]
}
llm_output = {
    "subtask_destination": "doorway",
    "subtask_landmark": "door",  # â†’ landmark_classes = ["door"]
    "completion_criteria": {...}
}

# ACT Phase  
vlm_input = {
    "images": [rgb_view, detection_view, local_map],
    "subtask": "Move to doorway"
}
vlm_output = {
    "action": "MOVE_FORWARD",  # or TURN_LEFT/RIGHT/STOP
    "reasoning": "Safe path ahead, door 3m away"
}

# REFLECT Phase
verify_input = {
    "360_scan": [updated_maps, new_detections],
    "criteria": ["door detected?", "distance<0.5m?", "correct position?"]
}
verify_output = {
    "is_completed": True,
    "next_subtask": {...}
}
```

---

## ğŸš€ Quick Start

### Prerequisites
```bash
# Assume Habitat-Sim/Lab v0.1.7 already installed on server
conda activate your-habitat-env
cd CA-Nav-code
```

### 1. Configure API Keys
```bash
cd vlnce_baselines/vlm/

# Create LLM config
cp llm_config.yaml.template llm_config.yaml
# Edit: add your OpenAI/Anthropic API key

# Create VLM config  
cp vlm_config.yaml.template vlm_config.yaml
# Edit: add your API key

# Example:
# api_type: "openai"
# api_key: "sk-..."
# model: "gpt-4o"
```

### 2. Run Navigation
```bash
# Single episode test
python run_vlm_navigation.py \
    --episode-id 0 \
    --split val_seen \
    --max-steps 500

# Batch evaluation
bash run_r2r/interactive_navigation.sh
```

### 3. Check Results
```bash
ls results/episode_0/
# â”œâ”€â”€ rgb/              # First-person views
# â”œâ”€â”€ detection/        # Object detection
# â”œâ”€â”€ global_map/       # Semantic maps
# â”œâ”€â”€ local_map/        # Local maps
# â””â”€â”€ vlm/
#     â”œâ”€â”€ observations/ # Stitched views
#     â”œâ”€â”€ subtasks/     # Subtask logs
#     â””â”€â”€ navigation.gif
```

---

## ğŸ“‚ Project Structure

```
CA-Nav-code/
â”œâ”€â”€ vlnce_baselines/
â”‚   â”œâ”€â”€ vlm/                           # Core VLM modules
â”‚   â”‚   â”œâ”€â”€ thinking.py                # LLM planner (THOUGHT)
â”‚   â”‚   â”œâ”€â”€ action.py                  # VLM executor (ACT)
â”‚   â”‚   â”œâ”€â”€ prompts.py                 # LLM prompts
â”‚   â”‚   â”œâ”€â”€ action_prompt.py           # VLM prompts
â”‚   â”‚   â””â”€â”€ api_client.py              # API wrapper
â”‚   â”œâ”€â”€ vlm_navigation_controller.py   # Main controller
â”‚   â”œâ”€â”€ detection/grounded_sam.py      # Object detection
â”‚   â”œâ”€â”€ mapping/mapper.py              # Semantic mapping
â”‚   â””â”€â”€ visualization/visualizer.py    # Visualization
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ å·¥ä½œæµç¨‹å›¾.md                   # Detailed workflow
â”‚   â””â”€â”€ å»ºå›¾æœºåˆ¶è¯´æ˜.md                 # Mapping mechanism
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Configuration

### Detection Classes
```python
# vlnce_baselines/config_system/constants.py
mapping_classes = [
    'floor', 'wall', 'door', 'bed', 'sofa', 'chair', 
    'table', 'desk', 'cabinet', 'tv', 'toilet', ...
]  # 25 common indoor objects

landmark_classes = []  # Dynamically updated per subtask
```

### Action Parameters
```python
TURN_ANGLE = 30        # degrees (12 steps = 360Â°)
MOVE_DISTANCE = 0.25   # meters
```

---

## ğŸ”‘ Key Mechanisms

### 1. Dynamic Landmark Tracking
```python
# Subtask 1
subtask_1 = {"subtask_landmark": "door"}
â†’ landmark_classes = ["door"]
â†’ Map: Purple "door" markers + Orange trajectory

# Subtask completes
mapper.clear_trajectory()

# Subtask 2  
subtask_2 = {"subtask_landmark": "sofa"}
â†’ landmark_classes = ["sofa"]  # Replaces "door"
â†’ Map: Purple "sofa" markers + New trajectory
```

### 2. Three-Image Action Guidance
```
VLM receives 3 images:
1. RGB View: First-person perspective
2. Detection View: Bounding boxes with labels
3. Local Map: Top-down semantic map
   â€¢ Green: Safe floor
   â€¢ Black: Obstacles (AVOID)
   â€¢ Orange: Trajectory
   â€¢ Blue: Field of view
```

### 3. 360Â° Contextual Awareness
```python
# Before each REFLECT phase
look_around_and_collect()  # 12 steps Ã— 30Â° = 360Â°
# Captures: Front (0Â°), Left (90Â°), Back (180Â°), Right (270Â°)
# Updates: Semantic map + Detected objects
```

---

## ğŸ“Š Performance

| Metric | Value | Note |
|--------|-------|------|
| Token Usage | 60k-120k | per episode (GPT-4o) |
| API Calls | 50-100 | LLM + VLM |
| Cost | $0.01-0.05 | per episode |
| Runtime | 5-10 min | 50-100 actions |

---

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| `FileNotFoundError: config.yaml` | Create from `.template` files |
| API call fails | Check API key and network |
| No landmarks on map | Verify `landmark_classes` updated |
| Trajectory accumulates | Confirm `clear_trajectory()` called |

---

## ğŸ“š Documentation

- **[å·¥ä½œæµç¨‹å›¾.md](docs/å·¥ä½œæµç¨‹å›¾.md)**: Detailed workflow with diagrams
- **[å»ºå›¾æœºåˆ¶è¯´æ˜.md](docs/å»ºå›¾æœºåˆ¶è¯´æ˜.md)**: Semantic mapping mechanism
- **[ç³»ç»Ÿè¯´æ˜æ–‡æ¡£.md](docs/ç³»ç»Ÿè¯´æ˜æ–‡æ¡£.md)**: System architecture (Chinese)

---

## ğŸ™ Acknowledgments

Built upon:
- **CA-Nav**: Constraint-aware navigation framework ([Paper](https://arxiv.org/abs/2412.10137))
- **Habitat**: Simulation platform
- **GroundedSAM**: Open-vocabulary detection

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE)

---

## ğŸ“§ Citation

If you use this code, please cite:

```bibtex
@article{chen2024constraint,
  title={Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments},
  author={Chen, Kehan and An, Dong and Huang, Yan and Xu, Rongtao and Su, Yifei and Ling, Yonggen and Reid, Ian and Wang, Liang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024}
}
```

---

**System Status**: âœ… Production Ready | **Version**: v1.0.0 | **Updated**: 2025-12-10
